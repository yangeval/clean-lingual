# 프로젝트 계획서: Clean-Lingual (악플 순화기)

공격적이고 유해한 문장(Toxic Text)을 입력받아, 본래의 의도는 유지하면서 정중하고 긍정적인 표현(Polite Text)으로 변환하는 전용 NMT 모델 구축 프로젝트입니다.

---

## 1. 프로젝트 개요
- **핵심 가치:** 단순 필터링(마스킹)을 넘어선 문장 재구성(Rephrasing). "언어의 삭제가 아닌 언어의 번역"
- **방식:** OpenNMT 프레임워크 기반의 Sequence-to-Sequence 학습
- **타겟:** 커뮤니티 관리 봇, 개인용 채팅 정화 필터, 세대 간 언어 순화 도구

## 2. 기술 스택 및 도구
- **NMT 프레임워크:** OpenNMT-py (PyTorch 기반)
- **모델 구조:** Transformer
- **최적화 엔진:** CTranslate2 (추론 속도 최적화 및 경량화, CPU 구동 가능)
- **독성 분류 모델:** KcELECTRA-base (게이트키퍼 역할)
- **데이터 증강:** LLM (GPT-4o 등)을 활용한 Synthetic Data 생성 (100개 -> 1,000개 확장)
- **토크나이저:** SentencePiece (BPE 방식)

## 3. 시스템 아키텍처: Pipeline 구조
정상 문장의 오염을 방지하고 연산 자원을 효율적으로 사용하기 위해 3단계 파이프라인을 채택합니다.

1. **[Step 1] 탐지 (Detection):** 고성능 독성 분류기(KcELECTRA)가 입력 문장의 독성 여부 판단.
2. **[Step 2] 판단 (Decision):** 독성 점수가 임계값(Threshold, 예: 0.6) 이상인 경우에만 NMT 모델로 전달. 미만인 경우 원문 즉시 반환.
3. **[Step 3] 변환 (Transformation):** 학습된 NMT 모델이 문맥을 유지하며 정중한 어투로 재구성.

## 4. 데이터셋 설계 및 증강 전략
- **데이터 구성:** `Source(악플)` - `Target(순화문)` 쌍
- **샘플 유형:**
    - 비난: "실력 실화냐? 진짜 개못하네." -> "실력을 조금 더 보완하시면 훨씬 멋진 플레이가 될 것 같아요."
    - 무시: "이걸 기사라고 썼냐?" -> "내용이 조금 더 보강된다면 독자들에게 더 큰 도움이 될 것 같습니다."
- **증강 포인트:** LLM 활용 시 "격식체", "해요체", "비즈니스 톤" 등 다양한 페르소나를 부여하여 데이터 다양성 확보.

## 5. 기존 방식 및 LLM 대비 우위
### ① 단순 필터링 대비
- **문맥 이해:** 특수문자 우회("시.발")나 비꼬는 표현 탐지에 강함.
- **연속성:** 대화 흐름을 끊지 않고 긍정적인 방향으로 유도함.

### ② 일반 LLM(GPT 등) 대비
- **비용:** API 호출당 비용이 아닌 고정 서버 비용으로 운영 (1/10 수준 절감).
- **속도:** CTranslate2 최적화를 통해 밀리초(ms) 단위 응답 가능.
- **보안:** 로컬/프라이빗 서버 운영 가능 (On-device AI 확장 용이).

## 6. 진행 로드맵 (4주)
- **[Week 1] 데이터 수집:** 실제 커뮤니티 사례 100개 수집 및 정제.
- **[Week 2] 데이터 확장 & 분류기 학습:** LLM 활용 1,000쌍 확보 및 KcELECTRA 기반 분류기 Fine-tuning.
- **[Week 3] NMT 모델 훈련:** OpenNMT-py 기반 Transformer 학습 및 CTranslate2 변환.
- **[Week 4] 평가 및 튜닝:** '순화도'와 '의도 보존율' 점검 및 UI 통합(확장 프로그램 등).

## 7. 향후 확장 가능성
- **CS 센터:** 감정 노동이 심한 상담원을 위한 실시간 필터링.
- **온라인 커뮤니티:** 디시인사이드, 레딧같은 게시판 형태의 커뮤니티에서 언어 순화.
- **게임 채팅:** 매너 점수 시스템과 연동한 능동적 중재.
