# ======================================================================================
# Clean-Lingual: Stage 1 Classifier All-in-One Colab Notebook Script
# ======================================================================================
# ì´ íŒŒì¼ì€ Google Colabì—ì„œ 0.5ë²„ì „ ë¶„ë¥˜ê¸° í•™ìŠµì˜ ëª¨ë“  ê³¼ì •ì„ ì¬í˜„í•˜ê¸° ìœ„í•œ ì½”ë“œ ëª¨ìŒì…ë‹ˆë‹¤.
# ê° ì„¹ì…˜ì„ ì½”ë©ì˜ ê°œë³„ ì…€(Cell)ë¡œ ë‚˜ëˆ„ì–´ ì‹¤í–‰í•˜ì‹œë©´ ë©ë‹ˆë‹¤.

# --------------------------------------------------------------------------------------
# [CELL 1] í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •
# --------------------------------------------------------------------------------------
"""
!pip install -q transformers[torch] datasets evaluate scikit-learn

import os
# ê°€ìƒí™˜ê²½ ë° ë¡œê¹… ê´€ë ¨ ì„¤ì •
os.environ["WANDB_DISABLED"] = "true" 
"""

# --------------------------------------------------------------------------------------
# [CELL 2] ë°ì´í„° ë¡œë“œ (GitHub í´ë¡  ë° ìµœì‹ í™”)
# --------------------------------------------------------------------------------------
"""
import os
if not os.path.exists('clean-lingual'):
    !git clone https://github.com/yangeval/clean-lingual.git

%cd clean-lingual
!git pull origin main
"""

# --------------------------------------------------------------------------------------
# [CELL 3] í•™ìŠµìš© ëª¨ë“ˆ ë° ì¤€ë¹„ (Dataset ì „ì²˜ë¦¬)
# --------------------------------------------------------------------------------------
"""
import pandas as pd
import torch
import numpy as np
import random
from datasets import Dataset, DatasetDict
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification, 
    TrainingArguments, 
    Trainer
)
import evaluate

# ê²½ë¡œ ì„¤ì •
DATA_PATH = "data/train_data/v0.5/"
MODEL_NAME = "beomi/KcELECTRA-base-v2022"

# ë°ì´í„° ë¡œë“œ
train_df = pd.read_csv(os.path.join(DATA_PATH, "train.tsv"), sep="\t")
valid_df = pd.read_csv(os.path.join(DATA_PATH, "valid.tsv"), sep="\t")
test_df = pd.read_csv(os.path.join(DATA_PATH, "test.tsv"), sep="\t")

def prepare_ds(df):
    return Dataset.from_dict({
        "text": df["source"].astype(str).tolist(),
        "label": df["action"].astype(int).tolist()
    })

dataset = DatasetDict({
    "train": prepare_ds(train_df),
    "valid": prepare_ds(valid_df),
    "test": prepare_ds(test_df)
})

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=128)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# í‰ê°€ ì§€í‘œ ì„¤ì •
metric = evaluate.load("f1")
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels, average="weighted")
"""

# --------------------------------------------------------------------------------------
# [CELL 4] ëª¨ë¸ í•™ìŠµ ì‹¤í–‰ (Fine-tuning)
# --------------------------------------------------------------------------------------
"""
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3)

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    eval_strategy="epoch", 
    save_strategy="epoch",
    learning_rate=2e-5,
    weight_decay=0.01,
    load_best_model_at_end=True,
    logging_steps=10,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["valid"],
    compute_metrics=compute_metrics,
)

trainer.train()
"""

# --------------------------------------------------------------------------------------
# [CELL 5] ìµœì¢… í…ŒìŠ¤íŠ¸ ë° ê²°ê³¼ ë¶„ì„
# --------------------------------------------------------------------------------------
"""
# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡
test_results = trainer.predict(tokenized_datasets["test"])
print("\n[!] ìµœì¢… í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­:", test_results.metrics)

# ëœë¤ ì‚¬ë¡€ í™•ì¸ (5ê°œ)
preds = np.argmax(test_results.predictions, axis=-1)
labels = test_results.label_ids
test_texts = dataset["test"]["text"]

print("\n" + "="*50)
print("ğŸ›¡ï¸ ì‹¤ì œ íŒë¡€ ë¶„ì„ (ëœë¤ 5ì„ )")
print("="*50)
for i in random.sample(range(len(preds)), 5):
    status = "âœ… ì •ë‹µ" if labels[i] == preds[i] else "âŒ ì˜¤ë‹µ"
    print(f"[{status}] ë¬¸ì¥: {test_texts[i]}")
    print(f"      (ì‹¤ì œ: {labels[i]} / ì˜ˆì¸¡: {preds[i]})\n")
"""

# --------------------------------------------------------------------------------------
# [CELL 6] ëª¨ë¸ ë‚´ë³´ë‚´ê¸° (Export & Download)
# --------------------------------------------------------------------------------------
"""
# ëª¨ë¸ ì €ì¥
model.save_pretrained("./final_model")
tokenizer.save_pretrained("./final_model")

# ì••ì¶• ë° ë‹¤ìš´ë¡œë“œ
!zip -r final_model_v0.5.zip ./final_model

from google.colab import files
files.download("final_model_v0.5.zip")
"""
