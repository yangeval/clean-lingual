# Clean-Lingual: Stage 1 Classifier Training Script (Colab Version)
# ëª¨ë¸: beomi/KcELECTRA-base-v2022
# ëª©ì : Action 0(Normal), 1(Block), 2(Purify) 3ë‹¨ê³„ ë¶„ë¥˜ í•™ìŠµ

import pandas as pd
import torch
import os
import numpy as np
from datasets import Dataset, DatasetDict
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification, 
    TrainingArguments, 
    Trainer
)
import evaluate

# 1. í™˜ê²½ ì„¤ì • ë° ë°ì´í„° ê²½ë¡œ
DATA_PATH = "data/train_data/v0.5/"
OUTPUT_DIR = "./results"
MODEL_SAVE_DIR = "./final_model"
MODEL_NAME = "beomi/KcELECTRA-base-v2022"

# WANDB ë¹„í™œì„±í™” (ë¡œê·¸ ê¸°ë¡ ìƒëµ)
os.environ["WANDB_DISABLED"] = "true"

def train():
    # 2. ë°ì´í„° ë¡œë“œ
    print("[*] ë¡œì»¬ ë°ì´í„° ë¡œë“œ ì¤‘...")
    try:
        train_df = pd.read_csv(os.path.join(DATA_PATH, "train.tsv"), sep="\t")
        valid_df = pd.read_csv(os.path.join(DATA_PATH, "valid.tsv"), sep="\t")
    except FileNotFoundError:
        print("[Error] ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return

    # 3. ë°ì´í„°ì…‹ ë³€í™˜ (HuggingFace Format)
    def prepare_ds(df):
        return Dataset.from_dict({
            "text": df["source"].astype(str).tolist(),
            "label": df["action"].astype(int).tolist()
        })

    dataset = DatasetDict({
        "train": prepare_ds(train_df),
        "valid": prepare_ds(valid_df)
    })

    # 4. í† í¬ë‚˜ì´ì € ë° ì „ì²˜ë¦¬
    print(f"[*] í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘: {MODEL_NAME}")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

    def tokenize_function(examples):
        return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=128)

    tokenized_datasets = dataset.map(tokenize_function, batched=True)

    # 5. í‰ê°€ ì§€í‘œ (F1 Score) ì„¤ì •
    metric = evaluate.load("f1")

    def compute_metrics(eval_pred):
        logits, labels = eval_pred
        predictions = np.argmax(logits, axis=-1)
        return metric.compute(predictions=predictions, references=labels, average="weighted")

    # 6. ëª¨ë¸ ë¡œë“œ (3ê°œ ë¼ë²¨ ë¶„ë¥˜ìš©)
    print(f"[*] ëª¨ë¸ ë¡œë“œ ì¤‘: {MODEL_NAME}")
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3)

    # 7. í•™ìŠµ ì¸ì ì„¤ì •
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        num_train_epochs=5,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        eval_strategy="epoch",  # ìµœì‹  ë¼ì´ë¸ŒëŸ¬ë¦¬ ê·œê²© (eval_strategy)
        save_strategy="epoch",
        learning_rate=2e-5,
        weight_decay=0.01,
        load_best_model_at_end=True,
        logging_steps=10,
    )

    # 8. Trainer ì´ˆê¸°í™” ë° í•™ìŠµ ì‹¤í–‰
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["valid"],
        compute_metrics=compute_metrics,
    )

    print("\n" + "="*50)
    print("ğŸš€ í•™ìŠµ ì‹œì‘ (KcELECTRA Classifier v0.5)")
    print("="*50)
    trainer.train()

    # 9. ëª¨ë¸ ìµœì¢… ì €ì¥
    print(f"\n[*] í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ ì €ì¥ ì¤‘: {MODEL_SAVE_DIR}")
    model.save_pretrained(MODEL_SAVE_DIR)
    tokenizer.save_pretrained(MODEL_SAVE_DIR)
    print("[!] ëª¨ë“  ê³¼ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    train()
